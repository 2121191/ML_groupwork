{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RGTAN-AFF 模型代码详解\n",
    "-  by 翁振昊\n",
    "\n",
    "- 基于论文《Enhancing Attribute-driven Fraud Detection with Risk-aware Graph Representation》的RGTAN模型进行的改进。\n",
    "\n",
    "- 在属性嵌入模块中，引入了注意力特征融合机制，以替代原始模型中对不同类别特征嵌入进行简单相加的方式。\n",
    "\n",
    "本文档是为了详细解释 `rgtan_aff_model.py` 文件中的主要类和方法,RGTAN相关的实验测试都在文件夹RGTAN_test里"
   ],
   "id": "13fc30d4ddd51e3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 导入必要的库",
   "id": "f40f7077295cac9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.255660Z",
     "start_time": "2025-05-31T07:29:02.094593Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim # 尽管此文件不直接用，但通常模型文件会包含\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl import function as fn\n",
    "from dgl.base import DGLError\n",
    "from dgl.nn.functional import edge_softmax # 用于 TransformerConv\n",
    "import numpy as np\n",
    "import pandas as pd # 用于 TransEmbedding 初始化时的df处理 (可选，取决于实际使用)\n",
    "from math import sqrt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. PosEncoding类：位置编码模块\n",
    "\n",
    "这个类用于生成位置编码，通常在处理序列数据或时间信息时使用，为模型提供关于位置或顺序的先验知识。在RGTAN的上下文中，它可能用于对交易的时间戳进行编码，但具体应用取决于 `TransEmbedding` 类如何使用它。\n",
    "\n",
    "**核心**: 利用不同频率的正弦和余弦函数来为不同位置创建独特的编码向量。"
   ],
   "id": "18c04d50d698aa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.271440Z",
     "start_time": "2025-05-31T07:29:04.255660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PosEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    位置编码模块\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, device, base=10000, bias=0):\n",
    "        \"\"\"\n",
    "        初始化位置编码组件\n",
    "        :param dim: 编码维度\n",
    "        :param device: 模型训练设备\n",
    "        :param base: 编码基数，用于计算不同频率\n",
    "        :param bias: 编码偏差，可以理解为相位的偏移\n",
    "        \"\"\"\n",
    "        super(PosEncoding, self).__init__()\n",
    "        p = []  # 存储周期的倒数 (1 / base^(2i/dim))\n",
    "        sft = [] # 存储相移 (0 或 pi/2)\n",
    "        for i in range(dim):\n",
    "            # 计算频率相关的指数项\n",
    "            b = (i - i % 2) / dim \n",
    "            p.append(base ** -b)\n",
    "            # 交替使用 sin 和 cos (通过相移实现)\n",
    "            if i % 2: # 奇数维度\n",
    "                sft.append(np.pi / 2.0 + bias) # cos(x) = sin(x + pi/2)\n",
    "            else: # 偶数维度\n",
    "                sft.append(bias) # sin(x)\n",
    "        \n",
    "        self.device = device\n",
    "        # 将相移和周期基数转换为tensor并移到指定设备\n",
    "        self.sft = torch.tensor(sft, dtype=torch.float32).view(1, -1).to(device)\n",
    "        self.base = torch.tensor(p, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "    def forward(self, pos):\n",
    "        \"\"\"\n",
    "        计算给定位置的位置编码\n",
    "        :param pos: 位置信息，可以是一个标量、列表或tensor\n",
    "        :return: 位置编码tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): # 位置编码通常是固定的，不参与梯度更新\n",
    "            if isinstance(pos, list): # 如果输入是列表，转换为tensor\n",
    "                pos = torch.tensor(pos, dtype=torch.float32).to(self.device)\n",
    "            pos = pos.view(-1, 1) # 确保pos是列向量 (num_positions, 1)\n",
    "            \n",
    "            # 核心公式: sin(pos / base_period + phase_shift)\n",
    "            # pos / self.base: 对应 w_k * t 中的 w_k * pos 部分\n",
    "            # self.sft: 对应相位\n",
    "            x = pos / self.base + self.sft\n",
    "            return torch.sin(x)"
   ],
   "id": "a5f4c2e228d7b18d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.TransformerConv类：门控时间图注意力层 (GTGA)\n",
    "这个类是论文中提出的 **门控时间图注意力 (Gated Temporal Graph Attention, GTGA)** 机制的核心实现。它用于在图节点之间传递和聚合信息，是典型的图注意力网络 (GAT) 的变体，并加入了门控机制和残差连接。\n",
    "\n",
    "**主要功能为：**:\n",
    "- **多头注意力**: 计算节点间的多头注意力权重，使模型能关注到更重要的邻居。\n",
    "- **消息聚合**: 根据注意力权重加权聚合邻居节点的特征。\n",
    "- **门控机制**: 动态地平衡聚合后的信息和节点自身的原始信息。\n",
    "- **残差连接与层归一化**: 帮助稳定训练过程，缓解梯度消失问题。\n",
    "\n",
    "*注意：虽然我们的RGTAN-AFF的创新点不在这个类，但为了模型的完整性，它依然是RGTAN架构的一部分。"
   ],
   "id": "d853acfbd3720d2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.297866Z",
     "start_time": "2025-05-31T07:29:04.271440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerConv(nn.Module):\n",
    "    \"\"\"\n",
    "    图注意力层，改编自DGL的TransformerConv，加入了门控机制等\n",
    "    对应论文中的 Gated Temporal Graph Attention (GTGA)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 bias=True,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 skip_feat=True, # 是否使用残差连接\n",
    "                 gated=True,     # 是否使用门控机制\n",
    "                 layer_norm=True,# 是否使用层归一化\n",
    "                 activation=nn.PReLU()):\n",
    "        super(TransformerConv, self).__init__()\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats) # 处理同构图和异构图的输入特征维度\n",
    "        self._out_feats = out_feats # 每个注意力头的输出特征维度\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree # 是否允许图中存在入度为0的节点\n",
    "        self._num_heads = num_heads # 注意力头的数量\n",
    "\n",
    "        # 线性变换层，用于计算Query, Key, Value\n",
    "        self.lin_query = nn.Linear(self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_key = nn.Linear(self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        self.lin_value = nn.Linear(self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "\n",
    "        # 残差连接的线性变换层\n",
    "        if skip_feat:\n",
    "            self.skip_feat = nn.Linear(self._in_src_feats, self._out_feats * self._num_heads, bias=bias)\n",
    "        else:\n",
    "            self.skip_feat = None\n",
    "        \n",
    "        # 门控机制的线性变换层\n",
    "        if gated:\n",
    "            # 输入是 skip_feat, rst, skip_feat - rst 拼接而成，所以维度是 3 * out_feats * num_heads\n",
    "            self.gate = nn.Linear(3 * self._out_feats * self._num_heads, 1, bias=bias)\n",
    "        else:\n",
    "            self.gate = None\n",
    "        \n",
    "        # 层归一化\n",
    "        if layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(self._out_feats * self._num_heads)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "        \n",
    "        self.activation = activation # 激活函数\n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        \"\"\"\n",
    "        :param graph: DGLGraph对象，当前计算子图\n",
    "        :param feat: 输入节点特征，形状 (num_nodes, in_feats)\n",
    "        :param get_attention: 是否返回注意力权重，默认为False\n",
    "        :return: 输出节点特征，形状 (num_dst_nodes, out_feats * num_heads)\n",
    "                 如果get_attention为True，则额外返回注意力权重\n",
    "        \"\"\"\n",
    "        with graph.local_scope(): # 使用local_scope确保对图的操作不影响外部\n",
    "            if not self._allow_zero_in_degree:\n",
    "                if (graph.in_degrees() == 0).any():\n",
    "                    raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "                                   'output for those nodes will be invalid. ')\n",
    "\n",
    "            if isinstance(feat, tuple): # 用于异构图，源节点和目标节点特征不同\n",
    "                h_src = feat[0]\n",
    "                h_dst = feat[1]\n",
    "            else: # 用于同构图，源节点和目标节点特征相同\n",
    "                h_src = feat\n",
    "                h_dst = h_src[:graph.number_of_dst_nodes()] # 取目标节点的特征\n",
    "\n",
    "            # 1. 线性变换得到 Q, K, V\n",
    "            # (num_nodes, in_feats) -> (num_nodes, num_heads * out_feats) -> (num_nodes, num_heads, out_feats)\n",
    "            q_src = self.lin_query(h_src).view(-1, self._num_heads, self._out_feats)\n",
    "            k_dst = self.lin_key(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "            v_src = self.lin_value(h_src).view(-1, self._num_heads, self._out_feats)\n",
    "            \n",
    "            # 将Q和V赋给源节点，K赋给目标节点 (在DGL消息传递中，Q和K来自不同侧)\n",
    "            graph.srcdata.update({'ft_q': q_src, 'ft_v': v_src})\n",
    "            graph.dstdata.update({'ft_k': k_dst})\n",
    "            \n",
    "            # 2. 计算注意力分数 (Attention Score)\n",
    "            # 使用DGL的内置函数 apply_edges(fn.u_dot_v(...)) 计算源节点Q和目标节点K的点积\n",
    "            # e_ij = (q_i * k_j) / sqrt(d_k)\n",
    "            graph.apply_edges(fn.u_dot_v('ft_q', 'ft_k', 'a')) # 'a' 是边的注意力原始分数\n",
    "            \n",
    "            # 3. Softmax归一化得到注意力权重\n",
    "            # graph.edata['a'] 形状是 (num_edges, num_heads, 1)\n",
    "            # 除以 sqrt(out_feats) 是Transformer中的缩放因子\n",
    "            graph.edata['sa'] = edge_softmax(graph, graph.edata['a'] / self._out_feats**0.5)\n",
    "            \n",
    "            # 4. 加权聚合邻居信息 (Value)\n",
    "            # 使用DGL的内置函数 update_all(fn.u_mul_e(...), fn.sum(...))\n",
    "            # 将源节点的Value (ft_v) 与边上的注意力权重 (sa) 相乘，然后对目标节点的所有入边进行求和\n",
    "            graph.update_all(fn.u_mul_e('ft_v', 'sa', 'attn_msg'), fn.sum('attn_msg', 'agg_u'))\n",
    "            \n",
    "            # 获取目标节点聚合后的特征\n",
    "            # (num_dst_nodes, num_heads, out_feats) -> (num_dst_nodes, num_heads * out_feats)\n",
    "            rst = graph.dstdata['agg_u'].reshape(-1, self._out_feats * self._num_heads)\n",
    "\n",
    "            # 5. 残差连接 (Skip Connection)\n",
    "            if self.skip_feat is not None:\n",
    "                skip_feat_transformed = self.skip_feat(h_dst) # 对原始目标节点特征进行线性变换\n",
    "                \n",
    "                # 6. 门控机制 (Gated Mechanism)\n",
    "                if self.gate is not None:\n",
    "                    # 拼接三种信息用于计算门控值\n",
    "                    gate_input = torch.cat([skip_feat_transformed, rst, skip_feat_transformed - rst], dim=-1)\n",
    "                    gate_val = torch.sigmoid(self.gate(gate_input))\n",
    "                    # 门控加权求和\n",
    "                    rst = gate_val * skip_feat_transformed + (1 - gate_val) * rst\n",
    "                else: # 如果没有门控，就是简单的相加\n",
    "                    rst = skip_feat_transformed + rst\n",
    "            \n",
    "            # 7. 层归一化 (Layer Normalization)\n",
    "            if self.layer_norm is not None:\n",
    "                rst = self.layer_norm(rst)\n",
    "            \n",
    "            # 8. 激活函数\n",
    "            if self.activation is not None:\n",
    "                rst = self.activation(rst)\n",
    "\n",
    "            if get_attention:\n",
    "                return rst, graph.edata['sa']\n",
    "            else:\n",
    "                return rst"
   ],
   "id": "c6c54e402d4f4bf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.Tabular1DCNN2类：表格数据的一维卷积网络\n",
    "\n",
    "这个类是论文中用于处理 **邻居风险感知特征** 的一个定制化的一维卷积神经网络。邻居风险感知特征通常是多个数值统计量如不同跳数的邻居度数、风险邻居计数等，可以看作是一个“表格”或一个“多通道的一维信号”。\n",
    "\n",
    "**主要目的**:\n",
    "- 从这些结构化的邻居统计特征中提取更深层次的模式。\n",
    "- 通过卷积操作捕捉特征之间的局部依赖关系。\n",
    "- 最终输出一个能代表邻居风险结构的嵌入向量。\n",
    "\n",
    "它包含多个卷积层、批归一化层、激活函数和残差连接，是一个相对复杂的特征提取器。"
   ],
   "id": "b00c6083d416012"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.313519Z",
     "start_time": "2025-05-31T07:29:04.297866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tabular1DCNN2(nn.Module):\n",
    "    \"\"\"\n",
    "    用于处理表格化邻居统计特征的一维卷积网络。\n",
    "    输入 x 的形状应为 (batch_size, num_neigh_stat_features)\n",
    "    在内部会 reshape 成 (batch_size, num_neigh_stat_features, 1) 或类似形式进行1D卷积，\n",
    "    或者直接在 (batch_size, num_neigh_stat_features, embed_dim) 上操作，\n",
    "    取决于具体的实现方式。这里的实现更像是把 embed_dim 作为“信号长度”。\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim: int,  # 邻居统计特征的数量 (例如，有多少种不同的统计值)\n",
    "                 embed_dim: int,  # 每个统计特征希望映射到的嵌入维度 (作为1D卷积的“信号长度”)\n",
    "                 K: int = 4,      # 卷积通道倍增因子\n",
    "                 dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim # 输入通道数\n",
    "        self.embed_dim = embed_dim # 可以理解为每个通道的“长度”\n",
    "        \n",
    "        # 初始全连接层，用于将输入扩展到一个较大的维度\n",
    "        self.hid_dim = input_dim * embed_dim * 2 \n",
    "        self.bn1 = nn.BatchNorm1d(input_dim) # 对原始输入特征进行BN\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense1 = nn.Linear(input_dim, self.hid_dim) # (B, num_stats) -> (B, hid_dim)\n",
    "\n",
    "        # 后续卷积操作的参数\n",
    "        self.cha_input = self.cha_output = input_dim # 保持通道数，但每个通道长度变为 embed_dim * 2\n",
    "        self.cha_hidden = (input_dim * K) // 2\n",
    "        self.sign_size1 = 2 * embed_dim # dense1输出reshape后的“信号长度”\n",
    "        self.sign_size2 = embed_dim     # 池化后的“信号长度”\n",
    "        self.K = K # 通道倍增因子\n",
    "\n",
    "        # 第一个卷积块 (分组卷积 + ReLU + 池化)\n",
    "        self.bn_cv1 = nn.BatchNorm1d(self.cha_input)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.cha_input,\n",
    "            out_channels=self.cha_input * self.K, # 通道数增加K倍\n",
    "            kernel_size=5,\n",
    "            padding=2,\n",
    "            groups=self.cha_input, # 分组卷积，每个输入通道独立卷积\n",
    "            bias=False\n",
    "        )\n",
    "        self.ave_pool1 = nn.AdaptiveAvgPool1d(self.sign_size2) # 池化到指定长度\n",
    "\n",
    "        # 第二个卷积块 (普通卷积 + ReLU + 残差连接)\n",
    "        self.bn_cv2 = nn.BatchNorm1d(self.cha_input * self.K)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=self.cha_input * self.K,\n",
    "            out_channels=self.cha_input * self.K, # 通道数不变\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        # 第三个卷积块\n",
    "        self.bn_cv3 = nn.BatchNorm1d(self.cha_input * self.K)\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=self.cha_input * self.K,\n",
    "            out_channels=self.cha_input * (self.K // 2), # 通道数减半\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        # 多个残差卷积块\n",
    "        self.bn_cvs = nn.ModuleList()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(6): # 6个残差块\n",
    "            self.bn_cvs.append(nn.BatchNorm1d(self.cha_input * (self.K // 2)))\n",
    "            self.convs.append(nn.Conv1d(\n",
    "                in_channels=self.cha_input * (self.K // 2),\n",
    "                out_channels=self.cha_input * (self.K // 2), # 通道数不变\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=True\n",
    "            ))\n",
    "\n",
    "        # 最后的卷积块，将通道数变回初始的 cha_output\n",
    "        self.bn_cv10 = nn.BatchNorm1d(self.cha_input * (self.K // 2))\n",
    "        self.conv10 = nn.Conv1d(\n",
    "            in_channels=self.cha_input * (self.K // 2),\n",
    "            out_channels=self.cha_output, # 通道数变回 input_dim\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 初始形状: (batch_size, input_dim) 代表 num_neigh_stat_features\n",
    "        \n",
    "        # 1. 初始全连接和变形\n",
    "        x = self.dropout1(self.bn1(x))\n",
    "        x = nn.functional.celu(self.dense1(x)) # (B, hid_dim)\n",
    "        # 变形以适应1D卷积: (B, cha_input, sign_size1) \n",
    "        # cha_input = input_dim, sign_size1 = 2 * embed_dim\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1) \n",
    "\n",
    "        # 2. 第一个卷积块\n",
    "        x = self.bn_cv1(x)\n",
    "        x = nn.functional.relu(self.conv1(x)) # (B, cha_input*K, sign_size1)\n",
    "        x = self.ave_pool1(x) # (B, cha_input*K, sign_size2)\n",
    "\n",
    "        # 3. 第二个卷积块 (带残差)\n",
    "        x_input_res = x # 保存残差连接的输入\n",
    "        x = self.dropout2(self.bn_cv2(x))\n",
    "        x = nn.functional.relu(self.conv2(x)) # (B, cha_input*K, sign_size2)\n",
    "        x = x + x_input_res # 残差连接\n",
    "\n",
    "        # 4. 第三个卷积块\n",
    "        x = self.bn_cv3(x)\n",
    "        x = nn.functional.relu(self.conv3(x)) # (B, cha_input*(K//2), sign_size2)\n",
    "\n",
    "        # 5. 多个残差卷积块\n",
    "        for i in range(6):\n",
    "            x_input_res_loop = x\n",
    "            x = self.bn_cvs[i](x)\n",
    "            x = nn.functional.relu(self.convs[i](x))\n",
    "            x = x + x_input_res_loop # 残差连接\n",
    "        \n",
    "        # 6. 最后一个卷积层\n",
    "        x = self.bn_cv10(x)\n",
    "        x = nn.functional.relu(self.conv10(x)) # (B, cha_output, sign_size2)\n",
    "                                             # cha_output = input_dim\n",
    "\n",
    "        # 输出形状: (batch_size, input_dim, embed_dim)\n",
    "        # 即为每个邻居统计特征都生成了一个 embed_dim 维度的向量\n",
    "        return x"
   ],
   "id": "958e894716887d3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.TransEmbedding类：交易特征嵌入与融合模块 (核心创新所在)\n",
    "\n",
    "这个类负责处理一笔交易中的多种原始特征，并将它们转换、融合成统一的向量表示，作为后续图神经网络层的输入。它主要处理三种信息：\n",
    "1.  **类别属性特征 (Categorical Features)**: 如卡类型、商户ID、交易地点等。\n",
    "2.  **邻居风险结构特征 (Neighbor Risk Features)**: 由 `Tabular1DCNN2` 处理后的邻居统计特征。\n",
    "3.  **时间特征 (Time Features)**\n",
    "\n",
    "**RGTAN-AFF 的核心创新点就在这个类中对“类别属性特征”的处理方式上。**\n",
    "\n",
    "**原始RGTAN**: 对每个类别特征分别进行嵌入和MLP处理后，直接相加。\n",
    "**RGTAN-AFF**: 引入了一个**轻量级的注意力网络 (`feature_attention_net`)**。\n",
    "   - 首先，每个类别特征仍旧分别进行嵌入和独立的MLP处理。\n",
    "   - 然后，不直接相加，而是将这些处理后的特征嵌入向量输入到 `feature_attention_net` 中，为每个特征计算一个注意力分数。\n",
    "   - 这些分数经过Softmax归一化后，形成注意力权重。\n",
    "   - 最后，用这些权重对原始的、经过MLP处理的特征嵌入向量进行加权求和，得到最终融合的类别特征向量 `cat_output`。\n",
    "\n",
    "**这样做可以：**:\n",
    "- **动态加权**: 模型可以根据当前交易的上下文，学习到不同类别特征的重要性，并赋予它们不同的权重。\n",
    "- **更强的表达能力**: 相比简单的无差别相加，注意力机制能更灵活地捕捉特征间的组合关系。\n",
    "- **潜在的可解释性**: 分析注意力权重可以帮助理解模型在做决策时更依赖哪些原始特征。"
   ],
   "id": "d9ddd0e54d6304e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.345467Z",
     "start_time": "2025-05-31T07:29:04.313519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    交易特征嵌入模块，融合了类别特征和邻居风险特征。\n",
    "    RGTAN-AFF 的创新点在此实现。\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 df=None, # 用于初始化类别特征Embedding时获取唯一值数量\n",
    "                 device='cpu',\n",
    "                 dropout=0.2,\n",
    "                 in_feats_dim=82, # 目标嵌入维度\n",
    "                 cat_features=None, # 类别特征的列名列表\n",
    "                 neigh_features: dict = None, # 邻居风险特征的列名或数量 (用于Tabular1DCNN2)\n",
    "                 att_head_num: int = 4, # 用于邻居风险特征自注意力的头数\n",
    "                 neighstat_uni_dim=64): # 似乎未使用\n",
    "        super(TransEmbedding, self).__init__()\n",
    "        \n",
    "        # 时间位置编码 (在此类的forward中未直接使用，可能为扩展预留)\n",
    "        self.time_pe = PosEncoding(dim=in_feats_dim, device=device, base=100)\n",
    "\n",
    "        # 1. 类别特征嵌入表\n",
    "        # 为每个类别特征创建一个独立的Embedding层\n",
    "        # df[col].unique() 用于获取该列唯一值的数量，从而确定Embedding词表大小\n",
    "        self.cat_table = nn.ModuleDict()\n",
    "        if df is not None and cat_features is not None:\n",
    "            self.cat_table = nn.ModuleDict({\n",
    "                col: nn.Embedding(max(df[col].astype(int).unique()) + 1, in_feats_dim).to(device) \n",
    "                for col in cat_features if col not in {\"Labels\", \"Time\"}\n",
    "            })\n",
    "        \n",
    "        # 2. 邻居风险特征处理模块\n",
    "        self.nei_table = None # Tabular1DCNN2\n",
    "        if isinstance(neigh_features, dict) or isinstance(neigh_features, pd.DataFrame): # 修改判断条件\n",
    "            num_neigh_stat_features = len(neigh_features.keys()) if isinstance(neigh_features, dict) else len(neigh_features.columns)\n",
    "            if num_neigh_stat_features > 0:\n",
    "                 self.nei_table = Tabular1DCNN2(input_dim=num_neigh_stat_features, embed_dim=in_feats_dim)\n",
    "\n",
    "        # 邻居风险特征的自注意力机制参数 (类似Transformer的自注意力)\n",
    "        self.att_head_num = att_head_num\n",
    "        self.att_head_size = int(in_feats_dim / att_head_num) if att_head_num > 0 else 0\n",
    "        self.total_head_size = in_feats_dim\n",
    "        self.lin_q = nn.Linear(in_feats_dim * (len(neigh_features.keys()) if neigh_features else 1) , self.total_head_size) # 修改维度以匹配Tabular1DCNN2的输出\n",
    "        self.lin_k = nn.Linear(in_feats_dim * (len(neigh_features.keys()) if neigh_features else 1), self.total_head_size)\n",
    "        self.lin_v = nn.Linear(in_feats_dim * (len(neigh_features.keys()) if neigh_features else 1), self.total_head_size)\n",
    "\n",
    "        self.lin_final = nn.Linear(in_feats_dim, in_feats_dim) # 自注意力输出后的线性层\n",
    "        self.layer_norm = nn.LayerNorm(in_feats_dim, eps=1e-8) # 层归一化\n",
    "\n",
    "        # 用于聚合邻居风险特征自注意力输出的MLP\n",
    "        self.neigh_mlp = nn.Linear(in_feats_dim, 1) # 输出一个标量？论文中是拼接，这里似乎是加权或选择\n",
    "\n",
    "        # 标签嵌入层 (用于风险传播，在RGTAN主类中使用，这里定义了以备不时之需)\n",
    "        self.label_table = nn.Embedding(3, in_feats_dim, padding_idx=2).to(device)\n",
    "        \n",
    "        self.emb_dict = None # 缓存cat_table\n",
    "        self.cat_features = cat_features if cat_features else []\n",
    "        self.neigh_features = neigh_features\n",
    "        \n",
    "        # 为每个类别特征创建一个独立的MLP层 (论文图2(b)中的MLP)\n",
    "        self.forward_mlp = nn.ModuleList(\n",
    "            [nn.Linear(in_feats_dim, in_feats_dim) for _ in range(len(self.cat_features))]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # --- RGTAN-AFF创新点---\n",
    "        # 定义一个用于计算各类别特征嵌入注意力权重的网络\n",
    "        # 输入是单个类别特征处理后的嵌入 (维度 in_feats_dim)\n",
    "        # 输出是一个标量，代表该特征的原始注意力分数\n",
    "        if self.cat_features: # 仅当有类别特征时才定义\n",
    "            self.feature_attention_net = nn.Sequential(\n",
    "                nn.Linear(in_feats_dim, in_feats_dim // 2),\n",
    "                nn.Tanh(), # 使用Tanh作为激活函数\n",
    "                nn.Linear(in_feats_dim // 2, 1) # 输出一个标量\n",
    "            )\n",
    "        else:\n",
    "            self.feature_attention_net = None\n",
    "        # --- RGTAN-AFF创新点---\n",
    "\n",
    "    def forward_emb(self, cat_feat: dict):\n",
    "        \"\"\"辅助函数：获取类别特征的原始嵌入\"\"\"\n",
    "        if self.emb_dict is None: # 缓存，避免重复赋值\n",
    "            self.emb_dict = self.cat_table\n",
    "        # cat_feat 是一个字典，key是特征名，value是该特征的ID tensor\n",
    "        support = {\n",
    "            col: self.emb_dict[col](cat_feat[col].long()) # 确保输入是LongTensor\n",
    "            for col in self.cat_features if col in self.emb_dict # 确保列名存在\n",
    "        }\n",
    "        return support\n",
    "\n",
    "    def transpose_for_scores(self, input_tensor):\n",
    "        \"\"\"辅助函数：变形张量以适应多头注意力计算 (B, L, H*D) -> (B, H, L, D)\"\"\"\n",
    "        new_x_shape = input_tensor.size()[:-1] + (self.att_head_num, self.att_head_size)\n",
    "        input_tensor = input_tensor.view(*new_x_shape)\n",
    "        return input_tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward_neigh_emb(self, neighstat_feat: dict):\n",
    "        \"\"\"处理邻居风险统计特征\"\"\"\n",
    "        if not self.nei_table or not neighstat_feat: # 检查 nei_table 是否初始化\n",
    "            return None, None\n",
    "\n",
    "        cols = list(neighstat_feat.keys())\n",
    "        # 将字典中的tensor堆叠起来，形状 (batch_size, num_neigh_stats)\n",
    "        tensor_list = [neighstat_feat[col] for col in cols]\n",
    "        neis_input_to_cnn = torch.stack(tensor_list, dim=1) \n",
    "        \n",
    "        # 1. 通过 Tabular1DCNN2 提取深度特征\n",
    "        # 输入 (B, num_stats), 输出 (B, num_stats, embed_dim)\n",
    "        input_tensor_for_attention = self.nei_table(neis_input_to_cnn) \n",
    "        \n",
    "        reshaped_input_tensor = input_tensor_for_attention.view(input_tensor_for_attention.size(0), -1)\n",
    "\n",
    "        mixed_q_layer = self.lin_q(reshaped_input_tensor) # (B, total_head_size)\n",
    "        mixed_k_layer = self.lin_k(reshaped_input_tensor) \n",
    "        mixed_v_layer = self.lin_v(reshaped_input_tensor) \n",
    "\n",
    "        q_layer = self.transpose_for_scores(mixed_q_layer.unsqueeze(1)) # (B, num_heads, 1, att_head_size)\n",
    "        k_layer = self.transpose_for_scores(mixed_k_layer.unsqueeze(1))\n",
    "        v_layer = self.transpose_for_scores(mixed_v_layer.unsqueeze(1)) \n",
    "\n",
    "        att_scores = torch.matmul(q_layer, k_layer.transpose(-1, -2)) # (B, num_heads, 1, 1)\n",
    "        att_scores = att_scores / sqrt(self.att_head_size if self.att_head_size > 0 else 1.0) # 缩放\n",
    "\n",
    "        att_probs = nn.Softmax(dim=-1)(att_scores) # (B, num_heads, 1, 1)\n",
    "        context_layer = torch.matmul(att_probs, v_layer) # (B, num_heads, 1, att_head_size)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() # (B, 1, num_heads, att_head_size)\n",
    "        new_context_shape = context_layer.size()[:-2] + (self.total_head_size,) # (B, 1, total_head_size)\n",
    "        context_layer = context_layer.view(*new_context_shape).squeeze(1) # (B, total_head_size)\n",
    "        \n",
    "        hidden_states = self.lin_final(context_layer) \n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "        return hidden_states, cols\n",
    "\n",
    "\n",
    "    def forward(self, cat_feat: dict, neighstat_feat: dict):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "        :param cat_feat: 字典，包含各类别特征的ID, e.g., {'feat1': tensor([...]), 'feat2': tensor([...])}\n",
    "        :param neighstat_feat: 字典，包含各邻居统计特征的值, e.g., {'deg1': tensor([...]), 'risk1': tensor([...])}\n",
    "        :return: cat_output (融合后的类别特征嵌入), nei_output (处理后的邻居风险特征嵌入)\n",
    "        \"\"\"\n",
    "        # 1. 处理类别特征\n",
    "        cat_output = None\n",
    "        if self.cat_features and self.feature_attention_net and cat_feat:\n",
    "            support = self.forward_emb(cat_feat) # 获取原始嵌入\n",
    "            \n",
    "            processed_embeddings = []\n",
    "            valid_feature_keys = [] # 存储实际处理的特征键，以防cat_feat不完整\n",
    "            for i, k_col_name in enumerate(self.cat_features): # 遍历预定义的类别特征顺序\n",
    "                if k_col_name in support:\n",
    "                    emb = support[k_col_name]\n",
    "                    # 每个类别特征先经过各自的MLP层\n",
    "                    # forward_mlp的索引应与cat_features的索引对应\n",
    "                    processed_emb = self.dropout(emb)\n",
    "                    processed_emb = self.forward_mlp[i](processed_emb)\n",
    "                    processed_embeddings.append(processed_emb)\n",
    "                    valid_feature_keys.append(k_col_name)\n",
    "\n",
    "            if processed_embeddings:\n",
    "                # (batch_size, num_valid_cat_features, embedding_dim)\n",
    "                stacked_embeddings = torch.stack(processed_embeddings, dim=1)\n",
    "                \n",
    "                # 计算每个特征的注意力分数\n",
    "                # (B, N_valid, D) -> (B, N_valid, 1)\n",
    "                attention_scores = self.feature_attention_net(stacked_embeddings)\n",
    "                \n",
    "                # Softmax归一化得到权重 (在特征维度 N_valid 上)\n",
    "                attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "                \n",
    "                # 加权求和\n",
    "                # (B, N_valid, D) * (B, N_valid, 1) -> 广播，逐元素相乘\n",
    "                # 然后在特征维度(dim=1)上求和，得到 (B, D)\n",
    "                cat_output = torch.sum(stacked_embeddings * attention_weights, dim=1)\n",
    "            else: # 如果没有有效的类别特征被处理\n",
    "                bs = 0\n",
    "                if cat_feat:\n",
    "                    first_key = next(iter(cat_feat))\n",
    "                    bs = cat_feat[first_key].size(0)\n",
    "                elif neighstat_feat:\n",
    "                    first_key = next(iter(neighstat_feat))\n",
    "                    bs = neighstat_feat[first_key].size(0)\n",
    "                \n",
    "                if bs > 0 :\n",
    "                    cat_output = torch.zeros(bs, self.in_feats_dim).to(self.device if hasattr(self, 'device') else 'cpu')\n",
    "                # else: cat_output 保持 None，或者根据后续逻辑处理\n",
    "\n",
    "        # 2. 处理邻居风险特征\n",
    "        nei_output_final = None # 初始化\n",
    "        if neighstat_feat and self.neigh_features : # 确保 neighstat_feat 和 self.neigh_features 都有效\n",
    "            nei_embs, _ = self.forward_neigh_emb(neighstat_feat) \n",
    "            if nei_embs is not None:\n",
    "                nei_output_final = nei_embs \n",
    "        \n",
    "        # 如果其中一个为空，需要提供一个零向量作为占位符，以确保后续操作（如拼接或相加）的维度正确\n",
    "        # 获取批大小 (batch_size)\n",
    "        current_batch_size = 0\n",
    "        if cat_output is not None:\n",
    "            current_batch_size = cat_output.size(0)\n",
    "        elif nei_output_final is not None:\n",
    "            current_batch_size = nei_output_final.size(0)\n",
    "        else: # 如果两者都为None，尝试从输入特征获取bs\n",
    "            if cat_feat:\n",
    "                first_key = next(iter(cat_feat))\n",
    "                current_batch_size = cat_feat[first_key].size(0)\n",
    "            elif neighstat_feat: # 确保neighstat_feat不为空字典\n",
    "                first_key = next(iter(neighstat_feat))\n",
    "                current_batch_size = neighstat_feat[first_key].size(0)\n",
    "        \n",
    "        # 如果 cat_output 是 None (例如没有类别特征或处理失败)，则创建零向量\n",
    "        if cat_output is None and current_batch_size > 0:\n",
    "            cat_output = torch.zeros(current_batch_size, self.in_feats_dim).to(self.device if hasattr(self, 'device') else 'cpu')\n",
    "\n",
    "        # 如果 nei_output_final 是 None (例如没有邻居特征或处理失败)，则创建零向量\n",
    "        if nei_output_final is None and current_batch_size > 0:\n",
    "            # 邻居特征的输出维度应与 cat_output 一致，即 in_feats_dim\n",
    "            nei_output_final = torch.zeros(current_batch_size, self.in_feats_dim).to(self.device if hasattr(self, 'device') else 'cpu')\n",
    "            pass \n",
    "\n",
    "        return cat_output, nei_output_final"
   ],
   "id": "fcf08607e387ef50",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. RGTAN类：模型主体架构\n",
    "\n",
    "这是实现 RGTAN (或 RGTAN-AFF) 完整功能的模型主类。它整合了前面定义的各个模块：\n",
    "- `TransEmbedding`: 用于获取初始的节点特征嵌入（包含类别特征融合和邻居风险特征）。\n",
    "- 标签嵌入 (Risk Propagation): 将交易标签也作为一种特征嵌入，并与节点属性融合。\n",
    "- 多层 `TransformerConv` (GTGA): 进行图上的消息传递和节点表示学习。\n",
    "- 最终的输出层: 一个MLP，用于基于学习到的节点表示进行欺诈预测。\n",
    "\n",
    "**RGTAN-AFF 与 RGTAN 的主要区别在于 `TransEmbedding` 模块的内部实现，而 `RGTAN` 这个主类的整体架构和流程保持不变。**"
   ],
   "id": "249795cff577acfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T07:29:04.377172Z",
     "start_time": "2025-05-31T07:29:04.345467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RGTAN(nn.Module):\n",
    "    \"\"\"\n",
    "    RGTAN 模型主体。\n",
    "    如果 TransEmbedding 是 RGTAN-AFF 版本，则此模型即为 RGTAN-AFF。\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_feats, # 原始数值特征的维度\n",
    "                 hidden_dim, # GNN隐层中每个头的维度\n",
    "                 n_layers, # GNN层数\n",
    "                 n_classes, # 输出类别数 (欺诈检测通常是2)\n",
    "                 heads, # GNN每层的注意力头数，列表形式，e.g., [4, 4] for 2 layers\n",
    "                 activation,\n",
    "                 skip_feat=True, # TransformerConv参数：是否用残差\n",
    "                 gated=True,     # TransformerConv参数：是否用门控\n",
    "                 layer_norm=True,# TransformerConv参数：是否用层归一化\n",
    "                 post_proc=True, # 是否在GNN输出后接一个后处理MLP\n",
    "                 n2v_feat=True,  # 是否使用 TransEmbedding 处理类别和邻居特征\n",
    "                 drop=None,      # Dropout率，列表形式 [input_dropout, hidden_dropout]\n",
    "                 ref_df=None,    # 用于 TransEmbedding 初始化 cat_table\n",
    "                 cat_features=None, # 类别特征列名列表\n",
    "                 neigh_features=None, # 邻居风险特征信息\n",
    "                 nei_att_head=4,    # TransEmbedding中邻居风险自注意力的头数\n",
    "                 device='cpu'):\n",
    "        super(RGTAN, self).__init__()\n",
    "        self.in_feats = in_feats # 原始数值特征维度\n",
    "        self.hidden_dim = hidden_dim # 注意力头的基础维度\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.heads = heads # GNN每层的头数列表\n",
    "        self.activation = activation\n",
    "        self.input_drop = nn.Dropout(drop[0]) if drop else nn.Identity()\n",
    "        self.drop = drop[1] if drop and len(drop) > 1 else 0.0\n",
    "        self.output_drop = nn.Dropout(self.drop)\n",
    "        self.device = device # 将device保存为成员变量\n",
    "\n",
    "        # 初始化 TransEmbedding 模块\n",
    "        self.n2v_mlp = None\n",
    "        self.actual_in_feats_for_gnn = in_feats # GNN的实际输入维度，会根据是否有TransEmbedding的输出调整\n",
    "        self.nei_feat_dim_from_trans_embedding = 0 # 从TransEmbedding来的邻居特征维度\n",
    "\n",
    "        if n2v_feat:\n",
    "            self.n2v_mlp = TransEmbedding(\n",
    "                df=ref_df, \n",
    "                device=device, \n",
    "                in_feats_dim=in_feats, # TransEmbedding内部输出的cat_embed维度与原始数值特征维度一致\n",
    "                cat_features=cat_features, \n",
    "                neigh_features=neigh_features, \n",
    "                att_head_num=nei_att_head\n",
    "            )\n",
    "\n",
    "            if neigh_features: # 假设如果用邻居特征，则会拼接\n",
    "                 self.actual_in_feats_for_gnn = in_feats + in_feats # X+cat_embed 和 neigh_embed 拼接\n",
    "                 self.nei_feat_dim_from_trans_embedding = in_feats \n",
    "            else:\n",
    "                 self.actual_in_feats_for_gnn = in_feats # 只有 X+cat_embed\n",
    "        else: # 如果不使用TransEmbedding，GNN输入就是原始数值特征\n",
    "            self.actual_in_feats_for_gnn = in_feats\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # 1. 标签嵌入层 (Risk Propagation)\n",
    "\n",
    "        self.layers.append(nn.Embedding(\n",
    "            n_classes + 1, self.actual_in_feats_for_gnn, padding_idx=n_classes))\n",
    "        \n",
    "        # 2. 融合标签嵌入与节点特征的MLP (论文图2(a)中的融合部分)\n",
    "        # 论文公式: h_tilde = MLP_node(h) + MLP_label(h_L_bar)\n",
    "\n",
    "        output_dim_for_label_fusion_mlp = self.hidden_dim * self.heads[0]\n",
    "\n",
    "        self.layers.append(nn.Linear(self.actual_in_feats_for_gnn, output_dim_for_label_fusion_mlp))\n",
    "        self.layers.append(nn.Linear(self.actual_in_feats_for_gnn, output_dim_for_label_fusion_mlp)) # 标签嵌入的输出维度也是actual_in_feats_for_gnn\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.BatchNorm1d(output_dim_for_label_fusion_mlp),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(self.drop),\n",
    "            nn.Linear(output_dim_for_label_fusion_mlp, self.actual_in_feats_for_gnn) # 映射回h的维度\n",
    "        ))\n",
    "\n",
    "        # 3. GNN层 (多层TransformerConv)\n",
    "        # 第一层GNN\n",
    "        self.layers.append(TransformerConv(\n",
    "            in_feats=self.actual_in_feats_for_gnn, # 经过特征融合和标签嵌入后的维度\n",
    "            out_feats=self.hidden_dim, # 每个头的输出维度\n",
    "            num_heads=self.heads[0],\n",
    "            skip_feat=skip_feat,\n",
    "            gated=gated,\n",
    "            layer_norm=layer_norm,\n",
    "            activation=self.activation\n",
    "        ))\n",
    "        \n",
    "        # 后续GNN层\n",
    "        # 输入维度是上一层GNN的输出维度 (hidden_dim * num_heads)\n",
    "        for l in range(1, self.n_layers): # 注意循环从1开始\n",
    "            self.layers.append(TransformerConv(\n",
    "                in_feats=self.hidden_dim * self.heads[l-1], \n",
    "                out_feats=self.hidden_dim,\n",
    "                num_heads=self.heads[l],\n",
    "                skip_feat=skip_feat,\n",
    "                gated=gated,\n",
    "                layer_norm=layer_norm,\n",
    "                activation=self.activation\n",
    "            ))\n",
    "        \n",
    "        # 4. 输出层 (Post-processing MLP)\n",
    "        if post_proc:\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim * self.heads[-1], self.hidden_dim * self.heads[-1]),\n",
    "                nn.BatchNorm1d(self.hidden_dim * self.heads[-1]),\n",
    "                nn.PReLU(),\n",
    "                nn.Dropout(self.drop),\n",
    "                nn.Linear(self.hidden_dim * self.heads[-1], self.n_classes)\n",
    "            ))\n",
    "        else:\n",
    "            self.layers.append(nn.Linear(self.hidden_dim * self.heads[-1], self.n_classes))\n",
    "\n",
    "    def forward(self, blocks, features, labels, n2v_feat=None, neighstat_feat=None):\n",
    "        \"\"\"\n",
    "        :param blocks: DGLGraph blocks for message passing in each GNN layer\n",
    "        :param features: 原始数值特征 (batch_nodes, in_feats)\n",
    "        :param labels: 节点标签 (用于风险传播) (batch_nodes,)\n",
    "        :param n2v_feat: 类别特征字典 (给TransEmbedding)\n",
    "        :param neighstat_feat: 邻居风险特征字典 (给TransEmbedding)\n",
    "        :return: logits (batch_target_nodes, n_classes)\n",
    "        \"\"\"\n",
    "        h = features # 初始 h 为原始数值特征 (X)\n",
    "\n",
    "        # 1. 通过TransEmbedding获取类别嵌入(cat_h)和邻居风险嵌入(nei_h)\n",
    "        if self.n2v_mlp is not None:\n",
    "            cat_h_from_trans, nei_h_from_trans = self.n2v_mlp(n2v_feat, neighstat_feat)\n",
    "            \n",
    "            if cat_h_from_trans is not None:\n",
    "                h = h + cat_h_from_trans # h = X + hA (论文中的 h_A)\n",
    "            \n",
    "            if nei_h_from_trans is not None: # 论文中是拼接 h_N\n",
    "                if h.size(-1) + nei_h_from_trans.size(-1) == self.actual_in_feats_for_gnn : # 假设拼接后的维度等于GNN输入维度\n",
    "                     h = torch.cat([h, nei_h_from_trans], dim=-1)\n",
    "                elif h.size(-1) == nei_h_from_trans.size(-1) : #如果维度相同，相加也是一种选择\n",
    "                     h = h + nei_h_from_trans # 这种方式意味着actual_in_feats_for_gnn计算时没有增加维度\n",
    "                else: # 维度不匹配，且不符合预期的拼接后维度，可能需要警告或错误\n",
    "                    if nei_h_from_trans.size(-1) == self.nei_feat_dim_from_trans_embedding and self.nei_feat_dim_from_trans_embedding > 0 :\n",
    "                         h = torch.cat([h, nei_h_from_trans], dim=-1)\n",
    "\n",
    "\n",
    "        # 2. 风险传播 (标签嵌入)\n",
    "        # self.layers[0] 是 nn.Embedding for labels\n",
    "        # self.layers[1,2,3] 是用于融合的MLP\n",
    "        label_embed_raw = self.input_drop(self.layers[0](labels)) # (B, actual_in_feats_for_gnn)\n",
    "        \n",
    "        fused_label_info = self.layers[1](h) + self.layers[2](label_embed_raw)\n",
    "        # 再过一层MLP\n",
    "        label_embed_processed = self.layers[3](fused_label_info) # (B, actual_in_feats_for_gnn)\n",
    "        h = h + label_embed_processed\n",
    "\n",
    "        # 3. 通过多层GNN (TransformerConv)\n",
    "        # GNN层从 self.layers[4] 开始\n",
    "        for l in range(self.n_layers):\n",
    "            # blocks[l] 是第l层GNN对应的计算子图\n",
    "            # self.layers[l+4] 是第l个TransformerConv层\n",
    "            h = self.output_drop(self.layers[l+4](blocks[l], h))\n",
    "            # h 的维度在第一层GNN后变为 (B, hidden_dim * heads[0])\n",
    "            # 后续层输入输出维度相应变化\n",
    "\n",
    "        # 4. 最终输出层\n",
    "        # self.layers[-1] 是最后的MLP分类器\n",
    "        logits = self.layers[-1](h) # (B, n_classes)\n",
    "\n",
    "        return logits"
   ],
   "id": "cd13e8446bae1713",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:10:22.542504Z",
     "start_time": "2025-05-31T08:10:21.475244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nbconvert import HTMLExporter\n",
    "import nbformat\n",
    "\n",
    "# 加载notebook文件\n",
    "with open('RGTAN-AFF_model.ipynb') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "html_exporter = HTMLExporter()\n",
    "html, resources = html_exporter.from_notebook_node(nb)\n",
    "\n",
    "# 写入HTML文件\n",
    "with open('RGTAN-AFF_model.html', 'w') as f:\n",
    "    f.write(html)"
   ],
   "id": "b53b7617b16858b4",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
