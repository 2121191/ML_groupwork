{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348900a0",
   "metadata": {},
   "source": [
    "### 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc9dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression # Explicitly import sklearn's LR\n",
    "from scipy.io import loadmat # To load .mat files\n",
    "import os # For file path handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fe8d3",
   "metadata": {},
   "source": [
    "### 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c8577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegressionModel = LogisticRegression\n",
    "DATA_DIR = './data'\n",
    "SFFSD_PATH = os.path.join(DATA_DIR, 'S-FFSD.csv')\n",
    "SFFSD_TIME_COLUMN = 'Time'\n",
    "SFFSD_LABEL_COLUMN = 'Labels'\n",
    "SFFSD_CATEGORICAL_COLUMNS = ['Source', 'Target', 'Location', 'Type']\n",
    "SFFSD_NUMERIC_COLUMNS = ['Time', 'Amount']\n",
    "\n",
    "AMAZON_PATH = os.path.join(DATA_DIR, 'Amazon.mat')\n",
    "YELPCHI_PATH = os.path.join(DATA_DIR, 'YelpChi.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3590c",
   "metadata": {},
   "source": [
    "### 设置训练S-FFSD数据集的超参数\n",
    "##### S-FFSD为半监督学习，训练时需要设置超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b982887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.9\n",
    "MAX_ITERATIONS = 10\n",
    "MIN_SAMPLES_PER_ITERATION = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe961a",
   "metadata": {},
   "source": [
    "### 对S-FFSD数据集进行预处理，以便进行半监督学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322e1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sffsd():\n",
    "    print(f\"\\n--- Preprocessing S-FFSD data from: {SFFSD_PATH} ---\")\n",
    "    data = pd.read_csv(SFFSD_PATH)\n",
    "\n",
    "    labelled_data = data[data[SFFSD_LABEL_COLUMN].isin([0, 1])].copy()\n",
    "    unlabelled_data = data[data[SFFSD_LABEL_COLUMN] == 2].copy()\n",
    "\n",
    "    print(f\"S-FFSD total: {len(data)}, labelled: {len(labelled_data)}, unlabelled: {len(unlabelled_data)}\")\n",
    "\n",
    "    data.sort_values(SFFSD_TIME_COLUMN, inplace=True)\n",
    "    labelled_data.sort_values(SFFSD_TIME_COLUMN, inplace=True)\n",
    "    unlabelled_data.sort_values(SFFSD_TIME_COLUMN, inplace=True)\n",
    "\n",
    "    all_data_for_preprocessing = pd.concat([labelled_data.drop(columns=[SFFSD_LABEL_COLUMN]),\n",
    "                                            unlabelled_data.drop(columns=[SFFSD_LABEL_COLUMN])], ignore_index=True)\n",
    "\n",
    "    high_card_cols = ['Source', 'Target']\n",
    "    for col in high_card_cols:\n",
    "        all_categories = pd.concat([labelled_data[col], unlabelled_data[col]]).unique()\n",
    "        le = LabelEncoder()\n",
    "        le.fit(all_categories)\n",
    "        all_data_for_preprocessing[col] = le.transform(all_data_for_preprocessing[col])\n",
    "\n",
    "    low_card_cols = ['Location', 'Type']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    encoded_features = encoder.fit_transform(all_data_for_preprocessing[low_card_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(low_card_cols), index=all_data_for_preprocessing.index)\n",
    "\n",
    "    processed_data_temp = all_data_for_preprocessing.drop(columns=SFFSD_CATEGORICAL_COLUMNS)\n",
    "    processed_data_full = pd.concat([processed_data_temp, encoded_df], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    processed_data_full[SFFSD_NUMERIC_COLUMNS] = scaler.fit_transform(processed_data_full[SFFSD_NUMERIC_COLUMNS])\n",
    "\n",
    "    processed_labelled_X = processed_data_full.iloc[:len(labelled_data)]\n",
    "    processed_unlabelled_X = processed_data_full.iloc[len(labelled_data):]\n",
    "    y_labelled = labelled_data[SFFSD_LABEL_COLUMN]\n",
    "\n",
    "    print(f\"Processed S-FFSD labelled X shape: {processed_labelled_X.shape}, y shape: {y_labelled.shape}\")\n",
    "    print(f\"Processed S-FFSD unlabelled X shape: {processed_unlabelled_X.shape}\")\n",
    "\n",
    "    return processed_labelled_X, y_labelled, processed_unlabelled_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72163148",
   "metadata": {},
   "source": [
    "### 加载和预处理 Amazon.mat与YelpChi.mat\n",
    "#### 这些文件被假设为完全监督，并具有 'features' 和 'label' 键。将特征转换为密集的 NumPy 数组并进行标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d1b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mat_data(file_path):\n",
    "    print(f\"\\n--- Preprocessing .mat data from: {file_path} ---\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    mat_data = loadmat(file_path)\n",
    "    features_sparse = mat_data['features']\n",
    "    labels_raw = mat_data['label'].flatten()\n",
    "\n",
    "    if hasattr(features_sparse, 'todense'):\n",
    "        features = features_sparse.todense().A\n",
    "    else:\n",
    "        features = features_sparse\n",
    "\n",
    "    labels = labels_raw.astype(np.int32)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    features_df = pd.DataFrame(features_scaled)\n",
    "\n",
    "    print(f\"Processed data shape: Features={features_df.shape}, Labels={labels.shape}\")\n",
    "    return features_df, pd.Series(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd07f9",
   "metadata": {},
   "source": [
    "### 实现基于逻辑回归的半监督学习中的自训练算法（Self-Training），并评估其性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1289de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_semisupervised_logistic_regression(\n",
    "    X_labelled, y_labelled, X_unlabelled,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD, max_iterations=MAX_ITERATIONS,\n",
    "    min_samples_per_iteration=MIN_SAMPLES_PER_ITERATION):\n",
    "    print(\"\\n=== Running Semi-Supervised Logistic Regression (S-FFSD) ===\")\n",
    "    X_train_initial, X_test_eval, y_train_initial, y_test_eval = train_test_split(\n",
    "        X_labelled, y_labelled, test_size=0.4, random_state=42, stratify=y_labelled\n",
    "    )\n",
    "    print(f\"Initial labelled train set: {X_train_initial.shape}, test set: {X_test_eval.shape}\")\n",
    "\n",
    "    X_train_self_training = X_train_initial.copy()\n",
    "    y_train_self_training = y_train_initial.copy()\n",
    "    current_unlabelled_X = X_unlabelled.copy()\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n--- Self-training iteration {iteration + 1}/{max_iterations} ---\")\n",
    "        if len(current_unlabelled_X) == 0:\n",
    "            print(\"All unlabelled data processed. Stopping self-training.\")\n",
    "            break\n",
    "\n",
    "        model = LogisticRegressionModel(max_iter=2000, solver='liblinear', random_state=42, n_jobs=-1)\n",
    "        print(f\"Current training set size: {len(X_train_self_training)}\")\n",
    "        model.fit(X_train_self_training, y_train_self_training)\n",
    "\n",
    "        if len(current_unlabelled_X) > 0:\n",
    "            unlabelled_probs = model.predict_proba(current_unlabelled_X)\n",
    "\n",
    "            confident_indices = (unlabelled_probs[:, 0] > confidence_threshold) | \\\n",
    "                                (unlabelled_probs[:, 1] > confidence_threshold)\n",
    "\n",
    "            confident_X = current_unlabelled_X[confident_indices]\n",
    "            confident_pseudo_labels = (unlabelled_probs[confident_indices, 1] > 0.5).astype(int)\n",
    "\n",
    "            print(f\"Iteration {iteration + 1}: Found {len(confident_X)} confident pseudo-labels.\")\n",
    "\n",
    "            if len(confident_X) < min_samples_per_iteration and iteration < max_iterations - 1:\n",
    "                print(f\"Too few confident pseudo-labels ({len(confident_X)} < {min_samples_per_iteration}), skipping this iteration.\")\n",
    "                continue\n",
    "\n",
    "            if len(confident_X) > 0:\n",
    "                X_train_self_training = pd.concat([X_train_self_training, confident_X], ignore_index=True)\n",
    "                y_train_self_training = pd.concat([y_train_self_training, pd.Series(confident_pseudo_labels)], ignore_index=True)\n",
    "                current_unlabelled_X = current_unlabelled_X[~confident_indices]\n",
    "                print(f\"New training set size: {len(X_train_self_training)}\")\n",
    "                print(f\"Remaining unlabelled pool size: {len(current_unlabelled_X)}\")\n",
    "            else:\n",
    "                print(\"No new confident pseudo-labels found this iteration. Stopping self-training.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Unlabelled data pool is empty. Stopping self-training.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n--- Final model training on complete semi-supervised set ---\")\n",
    "    final_model = LogisticRegressionModel(max_iter=2000, solver='liblinear', random_state=42, n_jobs=-1)\n",
    "    final_model.fit(X_train_self_training, y_train_self_training)\n",
    "    print(\"Final model training complete.\")\n",
    "\n",
    "    print(\"\\n--- Evaluating S-FFSD model on original test set ---\")\n",
    "    y_pred_eval = final_model.predict(X_test_eval)\n",
    "    y_prob_eval = final_model.predict_proba(X_test_eval)[:, 1]\n",
    "\n",
    "    cm = confusion_matrix(y_test_eval, y_pred_eval)\n",
    "    accuracy = accuracy_score(y_test_eval, y_pred_eval)\n",
    "    \n",
    "    f1 = f1_score(y_test_eval, y_pred_eval)\n",
    "    \n",
    "    avg_precision = average_precision_score(y_test_eval, y_prob_eval)\n",
    "\n",
    "    print(\"\\nS-FFSD Evaluation Metrics:\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "   \n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "   \n",
    "    print(f'Average Precision: {avg_precision:.4f}')\n",
    "\n",
    "    return {'accuracy': accuracy,  'f1': f1, 'avg_precision': avg_precision}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac901486",
   "metadata": {},
   "source": [
    "### 执行标准的有监督逻辑回归模型的训练和评估过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_supervised_logistic_regression(X, y, dataset_name):\n",
    "    print(f\"\\n=== Running Supervised Logistic Regression ({dataset_name}) ===\")\n",
    "    print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "    model = LogisticRegressionModel(max_iter=2000, solver='liblinear', random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_prob)\n",
    "\n",
    "    print(f\"\\n{dataset_name} Evaluation Metrics:\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Average Precision: {avg_precision:.4f}')\n",
    "\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'avg_precision': avg_precision}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc05216",
   "metadata": {},
   "source": [
    "### S-FFSD训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167bd060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing S-FFSD data from: ./data\\S-FFSD.csv ---\n",
      "S-FFSD total: 77881, labelled: 29643, unlabelled: 48238\n",
      "Processed S-FFSD labelled X shape: (29643, 464), y shape: (29643,)\n",
      "Processed S-FFSD unlabelled X shape: (48238, 464)\n",
      "\n",
      "=== Running Semi-Supervised Logistic Regression (S-FFSD) ===\n",
      "Initial labelled train set: (17785, 464), test set: (11858, 464)\n",
      "\n",
      "--- Self-training iteration 1/10 ---\n",
      "Current training set size: 17785\n",
      "Iteration 1: Found 9691 confident pseudo-labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training set size: 27476\n",
      "Remaining unlabelled pool size: 38547\n",
      "\n",
      "--- Self-training iteration 2/10 ---\n",
      "Current training set size: 27476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: Found 7397 confident pseudo-labels.\n",
      "New training set size: 34873\n",
      "Remaining unlabelled pool size: 31150\n",
      "\n",
      "--- Self-training iteration 3/10 ---\n",
      "Current training set size: 34873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Found 10848 confident pseudo-labels.\n",
      "New training set size: 45721\n",
      "Remaining unlabelled pool size: 20302\n",
      "\n",
      "--- Self-training iteration 4/10 ---\n",
      "Current training set size: 45721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: Found 11713 confident pseudo-labels.\n",
      "New training set size: 57434\n",
      "Remaining unlabelled pool size: 8589\n",
      "\n",
      "--- Self-training iteration 5/10 ---\n",
      "Current training set size: 57434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: Found 2836 confident pseudo-labels.\n",
      "New training set size: 60270\n",
      "Remaining unlabelled pool size: 5753\n",
      "\n",
      "--- Self-training iteration 6/10 ---\n",
      "Current training set size: 60270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6: Found 1550 confident pseudo-labels.\n",
      "New training set size: 61820\n",
      "Remaining unlabelled pool size: 4203\n",
      "\n",
      "--- Self-training iteration 7/10 ---\n",
      "Current training set size: 61820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7: Found 1586 confident pseudo-labels.\n",
      "New training set size: 63406\n",
      "Remaining unlabelled pool size: 2617\n",
      "\n",
      "--- Self-training iteration 8/10 ---\n",
      "Current training set size: 63406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8: Found 262 confident pseudo-labels.\n",
      "New training set size: 63668\n",
      "Remaining unlabelled pool size: 2355\n",
      "\n",
      "--- Self-training iteration 9/10 ---\n",
      "Current training set size: 63668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9: Found 32 confident pseudo-labels.\n",
      "Too few confident pseudo-labels (32 < 100), skipping this iteration.\n",
      "\n",
      "--- Self-training iteration 10/10 ---\n",
      "Current training set size: 63668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Found 32 confident pseudo-labels.\n",
      "New training set size: 63700\n",
      "Remaining unlabelled pool size: 2323\n",
      "\n",
      "--- Final model training on complete semi-supervised set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model training complete.\n",
      "\n",
      "--- Evaluating S-FFSD model on original test set ---\n",
      "\n",
      "S-FFSD Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      " [[9637  118]\n",
      " [1196  907]]\n",
      "Accuracy: 0.8892\n",
      "F1 Score: 0.5799\n",
      "Average Precision: 0.5441\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "sffsd_X_labelled, sffsd_y_labelled, sffsd_X_unlabelled = preprocess_sffsd()\n",
    "sffsd_results = train_and_evaluate_semisupervised_logistic_regression(\n",
    "    sffsd_X_labelled, sffsd_y_labelled, sffsd_X_unlabelled\n",
    ")\n",
    "all_results['S-FFSD_SemiSupervised_LR'] = sffsd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd478caa",
   "metadata": {},
   "source": [
    "### Amazon训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c632b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing .mat data from: ./data\\Amazon.mat ---\n",
      "Processed data shape: Features=(11944, 25), Labels=(11944,)\n",
      "\n",
      "=== Running Supervised Logistic Regression (Amazon) ===\n",
      "Data shape: X=(11944, 25), y=(11944,)\n",
      "Train set: (8360, 25), Test set: (3584, 25)\n",
      "Model training complete.\n",
      "\n",
      "Amazon Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      " [[3323   15]\n",
      " [  58  188]]\n",
      "Accuracy: 0.9796\n",
      "F1 Score: 0.8374\n",
      "Average Precision: 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "amazon_features, amazon_labels = preprocess_mat_data(AMAZON_PATH)\n",
    "amazon_results = train_and_evaluate_supervised_logistic_regression(\n",
    "    amazon_features, amazon_labels, \"Amazon\"\n",
    ")\n",
    "all_results['Amazon_Supervised_LR'] = amazon_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a2ac9",
   "metadata": {},
   "source": [
    "### YelpChi训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5c726a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing .mat data from: ./data\\YelpChi.mat ---\n",
      "Processed data shape: Features=(45954, 32), Labels=(45954,)\n",
      "\n",
      "=== Running Supervised Logistic Regression (YelpChi) ===\n",
      "Data shape: X=(45954, 32), y=(45954,)\n",
      "Train set: (32167, 32), Test set: (13787, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lxy\\.conda\\envs\\torch_env_1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "YelpChi Evaluation Metrics:\n",
      "Confusion Matrix:\n",
      " [[11634   150]\n",
      " [ 1745   258]]\n",
      "Accuracy: 0.8626\n",
      "F1 Score: 0.2140\n",
      "Average Precision: 0.4042\n"
     ]
    }
   ],
   "source": [
    "yelpchi_features, yelpchi_labels = preprocess_mat_data(YELPCHI_PATH)\n",
    "yelpchi_results = train_and_evaluate_supervised_logistic_regression(\n",
    "    yelpchi_features, yelpchi_labels, \"YelpChi\"\n",
    ")\n",
    "all_results['YelpChi_Supervised_LR'] = yelpchi_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71989c9",
   "metadata": {},
   "source": [
    "### 汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0508f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: S-FFSD_SemiSupervised_LR\n",
      "  Accuracy: 0.8892\n",
      "  F1: 0.5799\n",
      "  Avg Precision: 0.5441\n",
      "\n",
      "Dataset: Amazon_Supervised_LR\n",
      "  Accuracy: 0.9796\n",
      "  F1: 0.8374\n",
      "  Avg Precision: 0.8486\n",
      "\n",
      "Dataset: YelpChi_Supervised_LR\n",
      "  Accuracy: 0.8626\n",
      "  F1: 0.2140\n",
      "  Avg Precision: 0.4042\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, metrics in all_results.items():\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29e00831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbconvert import HTMLExporter\n",
    "import nbformat\n",
    " \n",
    "# 加载notebook文件\n",
    "with open('LogisticRegression_liuxiyuan.ipynb') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    " \n",
    "# 创建HTML导出器实例\n",
    "html_exporter = HTMLExporter()\n",
    "html, resources = html_exporter.from_notebook_node(nb)\n",
    " \n",
    "# 写入HTML文件\n",
    "with open('LogisticRegression_lxy.html', 'w') as f:\n",
    "    f.write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
