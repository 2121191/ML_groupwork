{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcce1ae-09da-4f1a-8cc1-e8dd450dc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test/svm_train_test.py\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c27d6-66e9-4681-9f43-a5a60b3b2db5",
   "metadata": {},
   "source": [
    "核心库：\n",
    "pandas/numpy：数据处理与数值计算。\n",
    "scikit-learn：提供 SVM 模型、数据划分、评估指标等工具。\n",
    "scipy：稀疏矩阵操作（csr_matrix）与.mat文件加载。\n",
    "HashingVectorizer：高基数特征哈希编码，避免维度爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7455e9-8cea-43b2-bb73-4219b2680b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接硬编码项目根目录\n",
    "PROJECT_ROOT = \"E:/Finance_fraud/Finance_fraud\"\n",
    "\n",
    "# 验证项目根目录\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    raise ValueError(f\"项目根目录不存在: {PROJECT_ROOT}\")\n",
    "\n",
    "# 将项目根目录添加到系统路径\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "print(f\"已添加项目根目录到路径: {PROJECT_ROOT}\")\n",
    "\n",
    "# 定义数据目录\n",
    "DATADIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "print(f\"数据目录: {DATADIR}\")\n",
    "\n",
    "# 验证 models 目录存在\n",
    "models_dir = os.path.join(PROJECT_ROOT, \"models\")\n",
    "if not os.path.exists(models_dir):\n",
    "    raise FileNotFoundError(f\"models 目录不存在: {models_dir}\")\n",
    "\n",
    "# 导入模型\n",
    "from models.svm_model import get_svm_model\n",
    "print(f\"成功导入 get_svm_model 函数\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6375a-7024-4b64-a3ee-5ddbecfedd3b",
   "metadata": {},
   "source": [
    "Finance_fraud/                 # 项目根目录\n",
    "├── data/                      # 数据集存放目录\n",
    "│   ├── S-FFSD.csv             # 金融欺诈数据集\n",
    "│   ├── Amazon.mat             # Amazon评论数据集\n",
    "│   └── YelpChi.mat            # Yelp餐厅评论数据集\n",
    "├── models/                    # 模型定义目录\n",
    "│   └── svm_model.py           # SVM模型工厂函数\n",
    "└── train_test/                # 训练测试脚本目录\n",
    "    └── svm_train_test.py      # 主实验脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21576c-6df9-45d4-a134-82c752a21d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm_experiment():\n",
    "    print(\"\\n=== 开始运行SVM实验 ===\")\n",
    "    \n",
    "    from sklearn.svm import LinearSVC\n",
    "    model = LinearSVC(max_iter=10000, dual=False)\n",
    "    print(f\"使用SVM模型: {model}\")\n",
    "    \n",
    "    # S-FFSD数据集（已在preprocess中划分好训练集和测试集）\n",
    "    print(\"\\n--- S-FFSD数据集 ---\")\n",
    "    X_train, X_test, y_train, y_test = preprocess_sffsd()\n",
    "    sffsd_accuracy, sffsd_auc, sffsd_f1, sffsd_ap = train_and_test(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Amazon数据集（新增划分步骤）\n",
    "    print(\"\\n--- Amazon数据集 ---\")\n",
    "    amazon_features, amazon_labels = preprocess_amazon()\n",
    "    X_amz_train, X_amz_test, y_amz_train, y_amz_test = train_test_split(\n",
    "        amazon_features, amazon_labels, test_size=0.4, random_state=42\n",
    "    )\n",
    "    amazon_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "    amazon_accuracy, amazon_auc, amazon_f1, amazon_ap = train_and_test(\n",
    "        amazon_model, X_amz_train, y_amz_train, X_amz_test, y_amz_test\n",
    "    )\n",
    "\n",
    "    # YelpChi数据集（新增划分步骤）\n",
    "    print(\"\\n--- YelpChi数据集 ---\")\n",
    "    yelpchi_features, yelpchi_labels = preprocess_yelpchi()\n",
    "    X_yc_train, X_yc_test, y_yc_train, y_yc_test = train_test_split(\n",
    "        yelpchi_features, yelpchi_labels, test_size=0.4, random_state=42\n",
    "    )\n",
    "    yelpchi_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "    yelpchi_accuracy, yelpchi_auc, yelpchi_f1, yelpchi_ap = train_and_test(\n",
    "        yelpchi_model, X_yc_train, y_yc_train, X_yc_test, y_yc_test\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'S-FFSD': {'Accuracy': sffsd_accuracy, 'AUC': sffsd_auc, 'F1': sffsd_f1, 'AP': sffsd_ap},\n",
    "        'Amazon': {'Accuracy': amazon_accuracy, 'AUC': amazon_auc, 'F1': amazon_f1, 'AP': amazon_ap},\n",
    "        'YelpChi': {'Accuracy': yelpchi_accuracy, 'AUC': yelpchi_auc, 'F1': yelpchi_f1, 'AP': yelpchi_ap}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== 实验结果汇总 ===\")\n",
    "    for dataset, metrics in results.items():\n",
    "        print(f\"{dataset}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d992ea2-567b-4b7a-bbc3-965dd3d425fb",
   "metadata": {},
   "source": [
    "1. 数据划分策略\n",
    "S-FFSD：基于时间顺序划分（由preprocess_sffsd内部实现）\n",
    "前 60% 数据作为训练集\n",
    "后 40% 数据作为测试集\n",
    "Amazon/YelpChi：随机划分\n",
    "train_test_split(test_size=0.4, random_state=42)\n",
    "固定随机种子确保结果可复现\n",
    "2. 评估指标体系\n",
    "Accuracy：准确率\n",
    "AUC：ROC 曲线下面积（二分类或多分类宏平均）\n",
    "F1 Score：精确率与召回率的调和平均数（宏平均）\n",
    "AP (Average Precision)：平均精度，衡量排序质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7351ea5-a758-4823-80d0-7ffe6181de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sffsd():\n",
    "    file_path = os.path.join(DATADIR, 'S-FFSD.csv')\n",
    "    print(f\"尝试加载文件: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"S-FFSD数据形状: {data.shape}\")\n",
    "    \n",
    "    time_column = 'Time'\n",
    "    if time_column not in data.columns:\n",
    "        raise ValueError(f\"数据集必须包含时间列 '{time_column}'\")\n",
    "    \n",
    "    data = data.sort_values(by=time_column, ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    numeric_columns = data.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    unique_counts = {col: data[col].nunique() for col in categorical_columns}\n",
    "    print(\"\\n分类列唯一值数量:\")\n",
    "    for col, count in unique_counts.items():\n",
    "        print(f\"- {col}: {count}\")\n",
    "    \n",
    "    # 保留Source列，仅删除其他高基数列\n",
    "    high_cardinality_columns = [col for col, count in unique_counts.items() if count > 1000 and col != 'Source']\n",
    "    if high_cardinality_columns:\n",
    "        print(f\"\\n警告: 以下高基数列将被删除: {high_cardinality_columns}\")\n",
    "        categorical_columns = [col for col in categorical_columns if col not in high_cardinality_columns]\n",
    "    else:\n",
    "        print(\"\\n提示: 无高基数列需要删除\")\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # 处理Source列（哈希编码，移除input_type参数）\n",
    "    if 'Source' in data.columns:\n",
    "        data['Source'] = data['Source'].astype(str)  # 确保为字符串类型\n",
    "        hasher = HashingVectorizer(n_features=1000, alternate_sign=False)  # 正确参数\n",
    "        source_hash = hasher.transform(data['Source'])\n",
    "        features.append(source_hash)\n",
    "    \n",
    "    # 处理其他分类列（One-Hot编码）\n",
    "    other_categorical_columns = [col for col in categorical_columns if col != 'Source']\n",
    "    if other_categorical_columns:\n",
    "        encoder = OneHotEncoder(sparse_output=True)\n",
    "        encoded_features = encoder.fit_transform(data[other_categorical_columns])\n",
    "        features.append(encoded_features)\n",
    "    \n",
    "    # 处理数值列\n",
    "    if numeric_columns:\n",
    "        numeric_sparse = csr_matrix(data[numeric_columns].values)\n",
    "        features.append(numeric_sparse)\n",
    "    \n",
    "    features = hstack(features).tocsr() if features else csr_matrix([])\n",
    "    \n",
    "    # 处理标签\n",
    "    labels = data['Labels']\n",
    "    if labels.dtype == 'object':\n",
    "        print(\"警告: 标签列是字符串类型，尝试转换为数值类型\")\n",
    "        labels = pd.to_numeric(labels, errors='coerce')\n",
    "        valid_indices = labels.notna()\n",
    "        features = features[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "    \n",
    "    # 时间划分\n",
    "    total_samples = len(labels)\n",
    "    train_size = int(0.6 * total_samples)\n",
    "    X_train, X_test = features[:train_size], features[train_size:]\n",
    "    y_train, y_test = labels[:train_size], labels[train_size:]\n",
    "    \n",
    "    print(f\"划分后: 训练集特征={X_train.shape}, 测试集特征={X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec717ed3-cc33-485a-92f9-8dc1e4c2da3a",
   "metadata": {},
   "source": [
    "`preprocess_sffsd()`函数主要用于预处理S-FFSD金融欺诈数据集，首先加载数据并验证时间列存在性，按时间排序后分离数值列与分类列。针对分类列中的高基数特征（如Source），使用哈希编码将其转换为固定维度（1000维）的特征向量以避免维度爆炸，同时过滤掉其他唯一值超过1000的高基数列。对剩余分类列进行稀疏One-Hot编码，数值列转换为稀疏矩阵，最终合并所有特征形成稀疏矩阵。处理标签时将字符串类型转换为数值并过滤无效样本，最后按时间顺序划分训练集（60%）和测试集（40%），确保数据符合时间敏感型场景的建模需求，为后续SVM模型训练提供结构合理、特征高效的输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ab93a-307d-439a-83db-9a6b71072051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_amazon():\n",
    "    \"\"\"\n",
    "    预处理Amazon数据集\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATADIR, 'Amazon.mat')\n",
    "    print(f\"尝试加载文件: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
    "    \n",
    "    amz = loadmat(file_path)\n",
    "    feat_data = pd.DataFrame(amz['features'].todense().A)\n",
    "    labels = pd.DataFrame(amz['label'].flatten())[0]\n",
    "    print(f\"Amazon数据形状: 特征={feat_data.shape}, 标签={labels.shape}\")\n",
    "    return feat_data, labels\n",
    "\n",
    "def preprocess_yelpchi():\n",
    "    \"\"\"\n",
    "    预处理YelpChi数据集\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DATADIR, 'YelpChi.mat')\n",
    "    print(f\"尝试加载文件: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
    "    \n",
    "    yelp = loadmat(file_path)\n",
    "    feat_data = pd.DataFrame(yelp['features'].todense().A)\n",
    "    labels = pd.DataFrame(yelp['label'].flatten())[0]\n",
    "    print(f\"YelpChi数据形状: 特征={feat_data.shape}, 标签={labels.shape}\")\n",
    "    return feat_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fb5cc-a696-4a25-9abc-278a47f160cc",
   "metadata": {},
   "source": [
    "`preprocess_amazon()`和`preprocess_yelpchi()`函数用于预处理Amazon和YelpChi数据集，两者逻辑高度相似。函数首先拼接数据文件路径并尝试加载`.mat`格式文件，若文件不存在则抛出异常。成功加载后，使用`loadmat`解析文件内容，将特征矩阵（`features`）转换为 pandas DataFrame，标签向量（`label`）经展平后提取为一维序列。过程中打印数据形状信息以便调试，最终返回特征数据和标签。该预处理流程适用于密集型特征矩阵的场景，直接保留原始特征结构，为后续随机划分训练集与测试集提供基础，适用于非时间序列的分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439f2848-c5f6-4b85-ae10-b41ea79ebb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动SVM实验脚本...\n",
      "数据目录: E:/Finance_fraud/Finance_fraud\\data\n",
      "\n",
      "=== 开始运行SVM实验 ===\n",
      "使用SVM模型: LinearSVC(dual=False, max_iter=10000)\n",
      "\n",
      "--- S-FFSD数据集 ---\n",
      "尝试加载文件: E:/Finance_fraud/Finance_fraud\\data\\S-FFSD.csv\n",
      "S-FFSD数据形状: (77881, 7)\n",
      "\n",
      "分类列唯一值数量:\n",
      "- Source: 30346\n",
      "- Target: 886\n",
      "- Location: 296\n",
      "- Type: 166\n",
      "\n",
      "提示: 无高基数列需要删除\n",
      "划分后: 训练集特征=(46728, 2351), 测试集特征=(31153, 2351)\n",
      "训练数据: 特征形状=(46728, 2351), 标签形状=(46728,)\n",
      "评估指标: Accuracy=0.8983, AUC=0.8529, F1=0.6191, AP=0.7009\n",
      "\n",
      "--- Amazon数据集 ---\n",
      "尝试加载文件: E:/Finance_fraud/Finance_fraud\\data\\Amazon.mat\n",
      "Amazon数据形状: 特征=(11944, 25), 标签=(11944,)\n",
      "训练数据: 特征形状=(7166, 25), 标签形状=(7166,)\n",
      "评估指标: Accuracy=0.9753, AUC=0.8813, F1=0.8921, AP=0.7553\n",
      "\n",
      "--- YelpChi数据集 ---\n",
      "尝试加载文件: E:/Finance_fraud/Finance_fraud\\data\\YelpChi.mat\n",
      "YelpChi数据形状: 特征=(45954, 32), 标签=(45954,)\n",
      "训练数据: 特征形状=(27572, 32), 标签形状=(27572,)\n",
      "评估指标: Accuracy=0.8525, AUC=0.7526, F1=0.4602, AP=0.4168\n",
      "\n",
      "=== 实验结果汇总 ===\n",
      "S-FFSD:\n",
      "  Accuracy: 0.8983\n",
      "  AUC: 0.8529\n",
      "  F1: 0.6191\n",
      "  AP: 0.7009\n",
      "Amazon:\n",
      "  Accuracy: 0.9753\n",
      "  AUC: 0.8813\n",
      "  F1: 0.8921\n",
      "  AP: 0.7553\n",
      "YelpChi:\n",
      "  Accuracy: 0.8525\n",
      "  AUC: 0.7526\n",
      "  F1: 0.4602\n",
      "  AP: 0.4168\n",
      "\n",
      "实验完成!\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    训练并测试模型，处理二分类和多分类情况\n",
    "    \"\"\"\n",
    "    print(f\"训练数据: 特征形状={X_train.shape}, 标签形状={y_train.shape}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测类别\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 获取预测概率或决策函数值\n",
    "    try:\n",
    "        # 尝试获取预测概率（对于支持predict_proba的模型）\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        # 如果模型不支持predict_proba，则尝试使用decision_function\n",
    "        y_score = model.decision_function(X_test)\n",
    "    \n",
    "    # 检查标签是否为二分类\n",
    "    unique_labels = np.unique(y_train)\n",
    "    is_binary = len(unique_labels) == 2\n",
    "    \n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if is_binary:\n",
    "        # 二分类情况\n",
    "        if y_score.ndim > 1:\n",
    "            y_score = y_score[:, 1]  # 对于二分类，只取正类的分数\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        ap = average_precision_score(y_test, y_score)\n",
    "    else:\n",
    "        # 多分类情况\n",
    "        # 将标签转换为one-hot编码\n",
    "        y_test_bin = label_binarize(y_test, classes=unique_labels)\n",
    "        \n",
    "        # 计算宏平均AUC\n",
    "        auc = roc_auc_score(y_test_bin, y_score, multi_class='ovr', average='macro')\n",
    "        \n",
    "        # 计算宏平均AP\n",
    "        ap = average_precision_score(y_test_bin, y_score, average='macro')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"评估指标: Accuracy={accuracy:.4f}, AUC={auc:.4f}, F1={f1:.4f}, AP={ap:.4f}\")\n",
    "    return accuracy, auc, f1, ap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"启动SVM实验脚本...\")\n",
    "        print(f\"数据目录: {DATADIR}\")\n",
    "        results = run_svm_experiment()\n",
    "        print(\"\\n实验完成!\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f08eb-ac25-4b90-bad6-72bd7283ab9b",
   "metadata": {},
   "source": [
    "`train_and_test`函数用于训练和评估SVM模型，支持二分类和多分类场景。函数首先打印训练数据形状并拟合模型，然后生成预测结果和分数（概率或决策值）。通过判断标签唯一性确定分类类型：二分类时提取正类分数计算AUC和AP，多分类则将标签二值化后计算宏平均指标。最终返回准确率、AUC、F1分数和平均精度，并打印结果。主程序入口部分负责启动实验流程，处理异常并打印结果汇总，确保实验流程的完整性和健壮性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b76ed-3348-4e5b-946d-da7afc9acfa3",
   "metadata": {},
   "source": [
    "一、项目概述\n",
    "1. 项目目标\n",
    "实现基于支持向量机（SVM）的分类实验，支持多数据集预处理、模型训练与多指标评估，验证 SVM 在不同场景下的性能表现。\n",
    "2. 技术栈\n",
    "编程语言：Python\n",
    "关键库：\n",
    "scikit-learn：模型训练与评估（SVM、数据划分、指标计算）\n",
    "pandas/numpy：数据处理与数值计算\n",
    "scipy：稀疏矩阵操作与.mat文件加载\n",
    "HashingVectorizer：高基数特征哈希编码\n",
    "二、项目结构\n",
    "plaintext\n",
    "E:/Finance_fraud/Finance_fraud  \n",
    "├── data/               \n",
    "│   ├── S-FFSD.csv       \n",
    "│   ├── Amazon.mat        \n",
    "│   └── YelpChi.mat      \n",
    "├── models/               \n",
    "│   └── svm_model.py      \n",
    "├── train_test/        \n",
    "│   └── svm_train_test.py   \n",
    "└── requirements.txt    \n",
    "三、核心功能模块\n",
    "1. 数据预处理模块\n",
    "1.1 S-FFSD 数据集（时间序列处理）\n",
    "核心逻辑：\n",
    "时间排序：按Time列升序排列，确保前 60% 为训练集，后 40% 为测试集。\n",
    "高基数特征处理：\n",
    "过滤唯一值 > 1000 的分类列。\n",
    "对Source列使用哈希编码，固定维度为 1000，避免 One-Hot 编码的维度爆炸。\n",
    "特征编码：\n",
    "分类列：稀疏 One-Hot 编码。\n",
    "数值列：转换为稀疏矩阵（csr_matrix）。\n",
    "输出：稀疏特征矩阵与数值标签，按时间划分的训练集 / 测试集。\n",
    "1.2 Amazon/YelpChi 数据集（密集矩阵处理）\n",
    "核心逻辑：\n",
    "直接加载.mat文件，返回密集特征矩阵与标签。\n",
    "使用train_test_split随机划分训练集 / 测试集（测试集占比 40%）。\n",
    "2. 模型训练与评估模块\n",
    "2.1 模型选择\n",
    "数据集类型\t            模型选择\t                   原因\n",
    "S-FFSD（稀疏高维）\t    LinearSVC\t      支持稀疏输入，适合大规模数据\n",
    "Amazon/YelpChi（密集）\tSGDClassifier\t  支持增量学习，内存效率高\n",
    "2.2 评估指标\n",
    "二分类：Accuracy、AUC-ROC、F1 分数、平均精度（AP）。\n",
    "多分类：宏平均 AUC-ROC、宏平均 F1 分数、宏平均 AP（通过label_binarize转换为 One-Hot 编码计算）。\n",
    "3. 实验运行模块\n",
    "初始化模型，依次处理 S-FFSD、Amazon、YelpChi 数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b04540-b626-49fb-a27a-67d5f0a7a456",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
